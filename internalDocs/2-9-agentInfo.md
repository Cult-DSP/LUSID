EARLY DESIGN DOC 2/5

LUSID Scene v0.5 – Node Graph Over Time Specification
Introduction
LUSID Scene v0.5 extends the original MVP design into a time-sequenced scene graph for spatial audio. The new format encodes audio objects and related metadata as a graph of nodes with hierarchical IDs, captured across discrete time steps. This design draws upon existing standards (glTF’s animation structure, Dolby Atmos ADM object metadata, sonoPleth’s spatial JSON) and introduces LUSID-specific extensions (node grouping conventions and frame-by-frame snapshots). The result is a clear schema for representing an audio scene as a series of timed snapshots, each containing all active nodes and their state. The following specification details the node graph structure, timeline framing, coordinate and keyframe format (for sonoPleth compatibility), guidelines for converting continuous animations to discrete frames, and support for layering spectral or AI data alongside audio objects. We also note the origin of each design choice – whether inspired by glTF, ADM, sonoPleth, or unique to LUSID.
Node Graph Convention (Group.Hierarchy Indexing)
Structured Node IDs: LUSID v0.5 uses a hierarchical node identification scheme X.Y to organize the scene graph. Here X is the group ID (a logical grouping of related nodes) and Y is the hierarchy level within that group. By convention, Y=1 denotes a primary or parent node, and higher numbers (2, 3, …) denote child nodes attached to that parent. This yields a simple tree structure where each group forms a subgraph (with one root and optional children). For example, in group 1 we might have node 1.1 as an audio object and node 1.2 as a descriptor attached to that audio object. Another group 2 could have 2.1 as an audio object and 2.2 as an AI agent state for that source. Each group’s “.1” node is effectively the root (often the audio emitter), and its .2, .3 etc. are ancillary data nodes. This numeric scheme is an original LUSID extension that provides an easy way to reference and organize nodes, analogous to how glTF defines a node hierarchy with parent-child relationships but here enforced by naming convention. Node Types: Every node entry includes a type field indicating its role or data type. Common types include:

- audio_object – a primary audio source (e.g. an individual sound in the scene). This carries spatial coordinates and possibly audio-specific parameters. It parallels the Audio Object concept in Dolby Atmos (an audio track with positional metadata).
- spectral_features – a child node that carries audio analysis data (e.g. spectral centroid, flux, bandwidth) for the parent audio object. This allows pairing an audio object with descriptive features of its sound at each moment.
- agent_state – a child node representing state information from an AI agent or interactive system linked to the audio (for example, an NPC’s emotion or behavior state driving the sound).
- LFE – a special node type for a Low-Frequency Effects channel. This typically appears as a standalone group (e.g. node 3.1 with type LFE) representing the subwoofer track. Marking a node as LFE ensures it’s handled appropriately by renderers (e.g. routed directly to subwoofers rather than spatialized).
  Additional types or extensions can be defined as needed (for instance, a perceptual_tag node for subjective labels, etc.), but the above cover common use cases. By structuring the scene into groups of related nodes, LUSID v0.5 makes it easy to manage multi-faceted audio objects – each group can be seen as one logical “entity” (with position, content, and any metadata layers).
  Timestep-Based Scene Snapshots
  Instead of describing motion continuously with curves, LUSID v0.5 represents the scene as a sequence of discrete time-indexed frames (snapshots). Each frame captures the state of all active nodes at a specific timestamp. In JSON form, we introduce a top-level array (e.g. "frames": []) where each entry looks like:

{ "time": <timestamp>, "nodes": [ {... node 1.1 state...}, {... node 1.2 state...}, ... ] }
This timestep-based approach means that at every listed time point, the positions and attributes of every audio object and its child nodes are explicitly recorded. For example, a frame at time=0.0 might contain nodes 1.1 (audio_object with its coordinates), 1.2 (spectral features at that moment), 2.1 (another audio_object), 2.2 (its agent state), and 3.1 (LFE state). The next frame at time=1.0 would then give all those nodes’ updated states at 1.0 seconds, and so on. If a node ceases to be active (e.g. an object stops or is removed), it simply won’t appear in later frames (or can be marked inactive), whereas new nodes can appear in frames when they start. This design is akin to a scene graph over time – effectively a timeline of fully realized scene states. It contrasts with typical animation data where each object might carry its own sparse keyframes. Here, the scene is sampled at uniform or meaningful intervals, bundling all objects’ keyframes together per timestep. The advantage is clarity and synchronization: a renderer or simulation can iterate through the frames in order and know the exact state of everything at each step, without needing to resolve different timelines or continuous curves. This is especially useful for audio, where sample-accurate timing can be critical – the snapshot rate can be as fine-grained as needed (even per audio sample, if timeUnit is set to samples) to capture rapid changes. Essentially, we have “baked” the animation of the entire scene into a series of fixed snapshots for deterministic playback. Frame Rate and Data Size: The spec doesn’t mandate a fixed frame interval – frames can be spaced irregularly or at regular cadence, depending on the source data and required temporal precision. In practice, one might choose a uniform frame rate (e.g. every 20 ms or every video frame) or place frames at points where any node’s state changes. The snapshot approach can increase data size if very high resolution is used, but this can be mitigated (see the section on continuous-to-discrete conversion for guidance on optimizing frame density). Tools may also compress or externalize large timeline data similar to glTF’s buffer technique if needed (storing the sequence in binary and indexing it), but the primary format is JSON for clarity.
sonoPleth Compatibility: Coordinates, Keyframes, and Time Units
To integrate seamlessly with the sonoPleth spatialInstructions JSON, LUSID Scene v0.5 adopts compatible conventions for coordinates and timing. Key aspects of sonoPleth’s format are incorporated:

- Explicit Time Units: We include a global "timeUnit" field (along with an optional "sampleRate" if needed) to define how timestamps are interpreted. This follows sonoPleth’s approach, where timeUnit can be "seconds" (default), "samples", or "milliseconds". For example, if timeUnit is "samples", a timestamp of 48000 would correspond to 1 second at 48 kHz sample rate. By specifying this in the scene JSON, we ensure no ambiguity in timing – a practice recommended in the sonoPleth schema. LUSID frames can thus be sample-accurate when needed (e.g. timeUnit: "samples" with sampleRate matching the audio), or use a timebase like seconds for convenience. Renderers should always check and honor timeUnit so that frame times align exactly with audio playback.
- Cartesian Direction Vectors (cart): Each audio object’s position can be given as a unit Cartesian vector [x, y, z] in the same coordinate system used by sonoPleth. In this system, x is left-right (–1 to +1), y is back-front, and z is down-up; the vector represents direction from the listener and will be normalized to length 1 by the renderer. LUSID nodes of type audio_object support a cart field to store this direction. For example: "cart": [0.0, 1.0, 0.0] would mean the source is directly in front of the listener. Using cart vectors means the LUSID scene can be directly translated into sonoPleth’s sources format (which lists keyframes of cart vectors per source). If a LUSID frame contains multiple audio objects with cart coordinates, each can be mapped to a sonoPleth source entry at that time.
- Spherical Coordinates: In addition to (or instead of) cart, LUSID still supports spherical coordinates (azimuth, elevation, radius) for positions, as used in the original MVP. These are useful for human readability and for alignment with certain metadata standards (e.g. ADM uses azimuth/elevation angles). The azimuth (Az) is the horizontal angle and elevation (El) the vertical angle (with 0° meaning front and horizontal, by convention), and radius can indicate distance from the listener. The exact unit (degrees vs radians) can be defined in the schema; for consistency with speaker layouts, radians might be used, but this should be clarified. Regardless, if both spherical and cart are provided, they must represent the same direction (the conversion is defined: the cart vector is essentially (x,y,z) = spherical_to_cart(Az, El) on the unit sphere, with radius possibly used for distance effects). In many cases, radius might be fixed (e.g. 1.0) if actual distance scaling is not applied in the renderer. The inclusion of cart is primarily for compatibility with sonoPleth’s renderer input, while azimuth/elevation may be kept for readability or integration with other systems.
- Keyframe Structure Alignment: The overall data can be easily converted between LUSID’s frame-centric structure and sonoPleth’s source-centric structure. In sonoPleth JSON, each source has its own list of keyframes (time + cart vector). In LUSID, we have a master list of frames containing all sources. To output LUSID data for the sonoPleth engine, one would: (a) assign each audio_object node a unique source name (e.g. group 1’s audio_object might be named “src_1” or a descriptive name like “vocals”) and (b) gather that node’s cart vectors and times from each frame to form the array of {time, cart} entries. Because LUSID frames already include every active source at every relevant time, no information is lost – it’s a matter of regrouping by source. Conversely, if one starts with sonoPleth’s sources JSON, it can be converted to LUSID frames by taking the union of all keyframe times and constructing a frame at each such time containing all sources’ latest positions. The sample accuracy is preserved either way, as long as timeUnit and sampleRate are kept consistent. Furthermore, any source named "LFE" in sonoPleth has special handling (bypassing spatialization to go to subwoofers); in LUSID, a node of type LFE serves the same purpose. When exporting to sonoPleth, the LFE node would be emitted as a source named "LFE" (with typically no cart needed, since it’s not spatialized by VBAP/DBAP). This one-to-one mapping of concepts ensures that LUSID scenes can drive the sonoPleth renderer without ambiguity.
- Time and Animation Granularity: sonoPleth does not explicitly define interpolation between provided keyframes – it can be assumed linear interpolation in time or instantaneous jumps at keyframe boundaries (depending on the renderer’s design). With LUSID’s dense frame approach, a conservative route is to supply sufficiently frequent keyframes such that linear interpolation is acceptable or not needed at all. For instance, if a sound moves in a smooth curve, LUSID can sample that motion at small time increments and feed those points to sonoPleth, which will then effectively follow the curve. The timeUnit field helps here as well: for extremely fast motions tied to audio samples, using sample counts as time ensures the renderer receives updates at exactly the right samples. In summary, LUSID v0.5 is sonoPleth-compatible by design – it speaks the same “language” of time-keyed direction vectors and explicit time units, only organized in a frame-by-frame snapshot form.
  Transition from Continuous Keyframes to Discrete Snapshots
  In earlier animation paradigms (and formats like glTF), motion is represented by sparse keyframes and interpolation algorithms. For example, glTF 2.0 animation uses samplers that pair an array of input times with an array of output values (plus an interpolation mode). In that model, if an object’s position is keyframed at t=0 and t=5, the runtime linearly (or via spline) interpolates all in-between positions. This continuous approach is efficient for graphics but can be problematic for sample-accurate audio control or when merging many timelines. LUSID v0.5 shifts to using discrete timestep snapshots as the primary representation, essentially baking the interpolation out into concrete frames. Here we provide guidance on converting or designing animations in this new format:
- Baking Animations: If source data comes in a continuous form (e.g. a glTF file or parametric path), the recommendation is to sample that animation at a sufficient rate and produce explicit frames. For instance, if an audio object’s position is defined by a curve, one might generate a LUSID frame every 10 milliseconds along the curve, capturing the evaluated position at those points. The required sampling frequency depends on the motion’s complexity and audio needs – faster or complex movements require finer sampling. In extreme cases (e.g. an object position modulated at audio rate), one could sample every audio sample (with timeUnit: "samples" so that each frame corresponds to a sample index). The goal is that any significant change between frames is acceptably small so that linear interpolation (or even simple stepping) between frame points is audibly smooth. This process is analogous to the “baking” of animation in 3D tools, where procedural or sparse animations are turned into frame-by-frame data for game engines or simulators.
- Interpolation Reduction: By using snapshots, we avoid needing interpolation logic in the renderer – but the content creator must ensure the data density is high enough. If one cannot afford extremely dense frames for long periods, a hybrid approach can be taken: for example, use frames at key event boundaries and assume linear change between them for the renderer. (Future versions of LUSID could incorporate an interpolation hint per node, similar to glTF’s linear/step/spline indicators, but v0.5 emphasizes fully specified frames.) In practice, modest frame rates (like 50–100 Hz) often suffice for smooth spatial trajectories, unless Doppler or rapid oscillations demand more resolution.
- Data Size and Buffers: Discretizing everything can lead to large JSON files if the scene runs over long durations. To manage this, LUSID can leverage techniques from glTF’s buffer model. In glTF, large arrays of animation data (times or values) are typically stored in a binary buffer and referenced by index. LUSID could adopt a similar strategy: for example, store the timeline of cart vectors for a given source in an external binary and let the JSON just reference that buffer. However, implementing a full buffer schema is beyond the core spec and optional in v0.5. As a simpler measure, data can be compressed or delta-encoded if needed, or tools can split scenes by time. The key point is that the conceptual model treats animation as a list of concrete states, not as mathematical functions. This is a conscious design choice to prioritize determinism and simplicity in playback. It echoes how some game engines or audio engines operate on tick-based updates rather than analytic curves.
- Continuous to Discrete Example: Imagine an audio object moving in a circle continuously. In glTF, you might store a few key points and a spline to interpolate a smooth circle. In LUSID, you would sample points along the circle, e.g. every 15° of rotation per frame, and list each in the timeline. The resulting motion will follow those points. If the circle needs to be perfectly smooth, you increase the sample rate (more frames). On playback, since each frame explicitly states the position, the renderer simply updates the object to that position at the given time – no complex math or large in-memory curves needed at runtime. This is especially advantageous for real-time audio rendering, where you might drive a panner or spatializer sample-by-sample from a precomputed trajectory.
  In summary, transitioning to discrete snapshots may entail more data upfront, but it ensures precision and ease of integration. We reduce reliance on interpolation algorithms (which could differ between systems) and avoid ambiguity. The influence of glTF is still visible in that we allow interpolation during content creation and borrowing the idea of external buffers for efficiency, but at runtime, LUSID’s philosophy is closer to a recorded simulation or an audio automation curve that has been fully resolved into individual time steps.
  Layering Spectral, Perceptual, and Agent-State Data
  LUSID’s node graph supports multi-layered data so that each audio object can carry not just spatial coordinates, but also related metadata streams (spectral analysis, perceptual tags, AI state, etc.) in sync. Using the group hierarchy (X.Y IDs), we attach these layers as child nodes to the primary audio node:
- A node of type spectral_features might include fields such as centroid, flux, bandwidth, or other audio feature metrics calculated from the audio content at that time. For instance, at a given frame, an audio object node 1.1 could have a companion 1.2 node where centroid: 5120 Hz and flux: 0.3 describe the spectrum. Over time, as the sound’s timbre changes, these values per frame give a time-series of the spectral profile. This is invaluable for visualization or for driving effects – e.g. an animation that reacts to the music’s brightness can read these values from the scene graph rather than analyzing audio separately.
- A node of type agent_state can hold arbitrary state information about an entity related to the sound. For example, if the audio is dialogue from a game character, an agent_state node could carry the character’s emotional state (mood: "excited") or an ID of their target in the scene, etc.. If the audio is generated by an autonomous system, this could contain AI parameters at that time. By logging these alongside the audio’s spatial data, one can later correlate how the agent’s state influenced the sound or vice versa. It effectively time-stamps the agent’s internal context on the same timeline as the audio.
- Other potential layered node types could include perceptual_label (e.g. crowd noise segment classified as “cheering”), mix_info (like an automated gain or reverb parameter applied to that object), or analysis_tag (markers like beat onsets or phoneme timestamps). The scene graph is extensible – new node types can be defined, and as long as they have unique type names and defined fields, they can live in the JSON. Unrecognized types would simply be ignored by a renderer that doesn’t use them.
  Crucially, all these child nodes share the same timeline as their parent audio object because they appear in the same frames. This means each frame is a holistic snapshot: not only do we know where the sound is coming from, we also know what it “sounds like” (spectrally) and perhaps why or how it’s produced (agent state) at that exact moment. This aligns with LUSID’s goal of unifying audio with its rich metadata. Neither glTF nor ADM provide such layered metadata in their core – this is an innovative LUSID feature, allowing complex data to be bundled in one scene description. From an implementation perspective, layering is optional. Simple scenes may have only audio_object nodes (like a pure Atmos-like object list). But the design anticipates future use cases like AI-driven soundscapes or augmented reality, where having perceptual and state information in the scene description could enable smarter rendering. For example, a future renderer might read spectral features to dynamically adjust an equalizer, or use agent mood to tweak the mix (making an “angry” voice sound more intense). By including these layers in LUSID v0.5, we ensure the format can serve as a foundation for both audio rendering and analysis.
  Schema Outline and Example
  Below is a high-level schema outline for LUSID Scene v0.5 and an example snippet illustrating the format: Schema Outline:

{ "version": "0.5", "sampleRate": 48000, // (Required if timeUnit = "samples"; otherwise optional) "timeUnit": "seconds", // e.g. "seconds" | "samples" | "milliseconds" "frames": [ { "time": <number>, // Timestamp in the specified timeUnit "nodes": [ { "id": "X.Y", // e.g. "2.1", group and hierarchy index "type": "<node_type>",// e.g. "audio_object", "spectral_features", ... /* type-specific fields below */ }, /* ... more nodes ... */ ] }, /_ ... more frames ... _/ ] }
Each node object will have a structure determined by its type value. For example:

- If "type": "audio_object": it may include fields such as "azimuth": <num>, "elevation": <num>, "radius": <num> (for spherical coords) or "cart": [x,y,z] (for Cartesian direction) and possibly other audio metadata (gain, spread, etc., if those are defined in future). In this spec, position is the key piece of data for audio_object nodes.
- If "type": "spectral_features": possible fields could be "centroid": <num>, "flux": <num>, "bandwidth": <num>, etc., representing the spectral attributes at that time.
- If "type": "agent_state": this might have a generic "state": <any> or more specific fields (e.g. "mood": "happy", "target": "enemy_1"). The schema can allow a free-form object for agent state data since it might be domain-specific.
- If "type": "LFE": typically no positional fields (since LFE is non-directional). It could have a field for level or content ID if needed, but often the presence of this node is enough to signal that an LFE channel is active.
  All nodes share a common "id" and "type", but their additional fields vary by type. This could be formalized with a oneOf schema (each type having its own definition), but for brevity we describe it conceptually here. Example JSON Snippet: (illustrative scene with two audio objects and one LFE over two time steps)

{ "version": "0.5", "sampleRate": 48000, "timeUnit": "seconds", "frames": [ { "time": 0.0, "nodes": [ { "id": "1.1", "type": "audio_object", "azimuth": 0.0, "elevation": 0.0, "radius": 1.0 /_ cart equivalent would be [0,1,0] pointing front _/ }, { "id": "1.2", "type": "spectral_features", "centroid": 5000.0, "flux": 0.15 }, { "id": "2.1", "type": "audio_object", "azimuth": 1.57, "elevation": 0.0, "radius": 1.0 /_ ~90 degrees to the right _/ }, { "id": "2.2", "type": "agent_state", "mood": "calm" }, { "id": "3.1", "type": "LFE" } ] }, { "time": 1.0, "nodes": [ { "id": "1.1", "type": "audio_object", "azimuth": 0.7, "elevation": 0.0, "radius": 1.0 }, { "id": "1.2", "type": "spectral_features", "centroid": 5200.0, "flux": 0.10 }, { "id": "2.1", "type": "audio_object", "azimuth": -0.7, "elevation": 0.0, "radius": 1.0 }, { "id": "2.2", "type": "agent_state", "mood": "excited" }, { "id": "3.1", "type": "LFE" } ] } ] }
In this example, at time=0.0s we have: Group 1 = an audio object in front of the listener (az=0) with some spectral features; Group 2 = another audio object to the right (az≈90°) with an agent state; Group 3 = an LFE channel. By time=1.0s, object 1 moved slightly to the right (az≈0.7 rad ≈40°) and object 2 moved to the left (az≈-0.7 rad ≈-40°), their spectral/agent data updated, and the LFE is still present. A renderer stepping through this would pan the first sound from front-center to off-right, etc. If exporting to sonoPleth, we’d create two sources out of this: one for source1 with cart vectors corresponding to az 0→0.7, and one for source2 with cart from 1.57→-0.7, plus an LFE source. Note: The actual JSON should use consistent units and formatting (the above azimuths are in radians). One could also include "cart" in each audio_object node for direct use: e.g. instead of az/el, "cart": [0.0, 1.0, 0.0] for node 1.1 at time 0.0. The spec allows whichever is convenient, as long as it’s documented – the conversion between spherical and cartesian is straightforward.
Integration and Usage Guidelines
Using LUSID v0.5 with Renderers: A real-time renderer (such as sonoPleth or a custom engine) can use the LUSID scene in one of two ways. It might directly support the frame-by-frame JSON – reading the next frame at each time step and applying all the changes (similar to how one would process an audio automation envelope). In this mode, the engine would likely run on a fixed tick (if timeUnit is samples, one frame per sample is possible, though typically you might step at a lower rate). The engine should interpolate between frame values if the frame rate is low, or simply update at frame boundaries if high-resolution frames are provided. Because each frame is self-contained, synchronization is simple (no need to align separate timelines). This is essentially treating the LUSID data as a script for the scene over time. Alternatively, if a renderer expects a per-source keyframe format (like sonoPleth does), a preprocessing step can convert the LUSID scene. As described, one would iterate over frames and dispatch each audio_object’s data into individual source tracks. This conversion can be automated. The advantage of LUSID’s format is that it can serve as a hub: from it you can derive other formats (ADM XML, sonoPleth JSON, or even a glTF animation for visualization) because it contains complete information. Live vs Offline Use: LUSID v0.5 is suitable for offline design of audio scenes (e.g. an audio drama or game level) which are then executed by a renderer. But it can also support live systems if the frames are generated on the fly (for example, an interactive application could stream out frames in real-time as JSON or via an API, and a renderer applies them immediately). Because the format is timestamped, the consumer can buffer and schedule future frames precisely. The explicit timeUnit ensures that even if producer and consumer run at different rates, the timing remains unambiguous. Extensibility: Users integrating LUSID should note which parts of the spec are mandatory vs optional. It’s mandatory to follow the node ID scheme and include all audio objects and their positions in frames. It’s optional to include spherical coordinates (you could choose cart only) or to include extra node types like spectral features – those will simply be ignored by a renderer not programmed to use them. We recommend always including a version field (to handle future changes gracefully) and thoroughly documenting any custom node types you add for your application. Compatibility with Existing Metadata: If one has existing Dolby Atmos ADM metadata (e.g. an ADM BWF file), how would that map to LUSID? Typically, each Atmos audio object becomes a group in LUSID (with at least an audio_object node). ADM metadata for that object’s position over time would be translated into the frames under that node’s ID. (ADM describes object trajectories possibly in XML with time slots; those can be sampled into our frames). The LFE channel in Atmos (if present) would map to a LUSID LFE node. Since ADM also can carry gain and spread (“size” in Atmos terms), those could be added as additional fields in the audio_object node if needed (not explicitly in v0.5, but the schema could be extended to include width or distance parameters similar to Atmos definitions). The takeaway is that LUSID can encapsulate ADM-like data; one could even losslessly convert an ADM scene to LUSID and back, with the limitation that LUSID currently focuses on positional metadata and user-defined extra data (ADM’s detailed loudness or dialogue metadata might need future extension). Future Live Renderers: For any future audio engine or live renderer, the scene graph over time concept provides a robust framework. Such an engine can traverse the node graph to configure audio processing (e.g. instantiate an audio source for each audio_object node, perhaps an EQ filter for each spectral_features node if one wanted to apply dynamic EQ based on features, etc.), then step through time to update parameters. It’s essentially feeding a timeline of parameter updates. This approach scales from simple panning to complex interactive audio with analytics. We recommend that future renderers that adopt LUSID v0.5 ensure that they support:

- Dynamic object instantiation (as nodes appear/disappear across frames).
- Accurate timing (honoring timeUnit for when to apply frame updates).
- Ignore unknown types safely (so the system can evolve – e.g. a renderer that doesn’t use agent_state data should simply skip those nodes, while one that knows a certain agent_state field might react to it).
  Finally, it’s worth noting that storing a scene as discrete frames is also convenient for debugging and analysis. One can inspect or plot any parameter over time easily. This human-readability and direct correlation of multi-modal data is a strength of LUSID’s design.
  Design Lineage and Influences
  To clarify the origins of each part of LUSID Scene v0.5, we summarize how existing frameworks inspired this design:
- glTF 2.0 (Graphics Layered Transmission Format): LUSID borrows the concept of a node-based scene graph from glTF. In glTF, scenes are composed of nodes in a strict parent-child hierarchy. LUSID similarly uses a hierarchy (groups with children), though with a simpler numeric indexing instead of explicit child arrays. Furthermore, glTF’s animation system (inputs and outputs arrays with interpolation) motivated LUSID’s decision to move away from implicit interpolation – we saw the need to “bake” those curves into concrete samples for audio. The idea of using binary buffers for large data is also drawn from glTF’s approach to keep JSON lean; while not fully implemented in v0.5, this influence guides how we might handle heavy time-series efficiently. In short, glTF provided the structural backbone (nodes and possibly external buffers) but LUSID adapts it for the audio domain and discrete timing.
- Dolby Atmos & ADM (Audio Definition Model): Atmos introduced the industry to audio objects with metadata – sounds that carry position (azimuth, elevation, distance), gain, and spread, separate from channel-based beds. LUSID’s audio_object type is directly derived from this concept. We treat each such object as an independent entity that can be placed in 3D space, much like Atmos does in its ADM metadata. The presence of an LFE channel node and the idea of special-casing it also comes from the traditional role of LFE in surround formats (Atmos ADM marks the LFE track which is not spatialized, similar to our LFE node). ADM’s representation of timed metadata (usually as XML in a Broadcast Wave file) influenced our timeline structure as well, though ADM tends to use parametric descriptors and time segments rather than our snapshot method. We aligned coordinate definitions where possible (e.g., azimuth/elevation angles meaning the same as in ADM, which typically uses degrees). The notion of encapsulating a whole scene’s audio metadata in a single structured file is very much what ADM enables; LUSID attempts to do this in a more programmer-friendly JSON form and extend it with non-ADM data like spectral analysis. So, Atmos/ADM primarily contributed the audio object model and the emphasis on preserving object-based mix data.
- sonoPleth Spatial Instructions (JSON format): This format directly shaped LUSID v0.5’s approach to coordinates and timing. We incorporated the cart direction vectors per source and the explicit timeUnit with sample-rate context to guarantee we can feed LUSID scenes into the sonoPleth renderer without loss of information. The simplicity of sonoPleth’s JSON (each source -> list of {time, cart} points) is reflected in LUSID’s design, except transposed into a global frame view. Our decision to structure frames globally was a LUSID choice for easier scene snapshotting, but it’s fully compatible with source-by-source views. Additionally, sonoPleth’s handling of the LFE (by naming convention) was noted – LUSID instead uses an explicit node type for clarity, but we ensure an easy mapping to the name "LFE" in output. In essence, sonoPleth’s format influenced LUSID’s coordinate system, timing precision, and output interoperability.
- Original LUSID Extensions: The idea of numbered node grouping (X.Y) and the use of timestep snapshots are novel contributions of LUSID. These were not drawn from existing standards but arose from the needs of blending audio data with complex metadata in a unified timeline. The grouping scheme was created to keep related data together without complex object linking – a human-readable shorthand that also simplifies parsing. The frame-by-frame approach was inspired by practices in audio processing (where you often process data block by block or sample by sample) and in simulation (stepping through states), rather than by typical media formats. LUSID also introduced the concept of layering arbitrary analysis or AI data into the scene graph, which is unique – neither glTF, ADM, nor sonoPleth have a built-in concept for, say, spectral centroid or agent mood streams attached to objects. This extensibility reflects LUSID’s goal to serve not just as an interchange format but as a creative tool, enabling new kinds of audio-driven experiences.
  By clarifying these lineages, we ensure that implementers understand the rationale behind each part of the spec. We stand on the shoulders of giants (Khronos glTF for scene structure, EBU ADM for audio metadata, sonoPleth for real-time rendering needs) and extend those ideas to cover new ground needed for LUSID’s use cases.
  Conclusion
  The revised LUSID Scene v0.5 specification defines a comprehensive, time-aware scene graph for spatial audio and beyond. It marries the robust hierarchy of 3D scene graphs with the precision of audio metadata timelines, packaged in an accessible JSON schema. With node groups like “1.1, 1.2” linking audio objects to rich descriptions, and frames capturing every nuance at each moment, creators can script complex audio scenes knowing that all the data stays synchronized. We have ensured that this design plays well with existing frameworks – you can derive it from or convert it to formats like Dolby Atmos ADM and the sonoPleth renderer’s JSON, ensuring compatibility with current workflows. At the same time, LUSID v0.5 opens the door to new possibilities by including spectral and state information in the scene. This spec is intended to be clear and practical: schema sketches and examples have been provided to guide implementation. Going forward, a live renderer or any “LUSID-aware” tool can adopt this format to enable dynamic, interactive audio experiences with a rich understanding of the scene’s context. By transitioning to discrete time snapshots, we prioritize determinism and accuracy – crucial for audio – while still allowing high-level animation tools to be used upstream. We recommend using this spec as a foundation for any scenario that involves spatial audio objects with evolving properties over time. Sources: The development of LUSID Scene v0.5 was informed by the glTF 2.0 spec for node hierarchies and animation structure, the Dolby Atmos ADM model for object-based audio (notably the concept of audio objects with position metadata), the sonoPleth spatial audio JSON for direction vectors and timing semantics, and LUSID’s own prior concept notes for node indexing and frame layout. All these influences were blended to create a format that is both familiar in its building blocks and innovative in its integration of multi-dimensional audio data.

sample accuracy
